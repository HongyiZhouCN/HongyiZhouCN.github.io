<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Hongyi Zhou</title>
  
  <meta name="author" content="Hongyi Zhou">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Hongyi Zhou (Âë®ÂºòÊØÖ)</name>
              </p>
              <p>I am a PhD student in the <a href="https://irl.anthropomatik.kit.edu/">Intuitive Robots Lab (IRL)</a> at the Karlsruhe Institute of Technology (KIT), Germany.
                My research focuses on developing efficient and scalable methods for robot learning under the supervision of <a href="http://rudolf.intuitive-robots.net/">Rudolf Lioutikov</a>.
                <br>
                Before I start my PhD, I earned my Master‚Äôs degree in Mechatronics Engineering at KIT, where I completed my thesis on Episodic Reinforcement Learning in the <a href="https://alr.iar.kit.edu/">Autonomous Learning Robots Lab (ALR)</a>, supervised by <a href="https://alr.iar.kit.edu/21_65.php">Gerhard Neumann</a>. 
                Prior to that, I obtained my Bachelor's degree in Mechatronics Engineering from the Harbin Institute of Technology, China.
              </p>

              <p style="text-align:center">
                <a href="mailto:hongyi.zhou@kit.edu">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=W35-J2sAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/HongyiZhouCN">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/hongyi-zhou-9413b9242/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/IMG_7493.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/IMG_7500.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                 My primary research goal is to develop machine learning algorithms that enable the usage of robots in everyday life. 
                 Potential paths to reach this goal include imitation learning and reinforcement learning.
                 A key challenge for imitation learning is the difficulty of collecting high-quality demonstrations, which are costly and time-consuming. 
                 Reinforcement learning, on the other hand, is usually inefficient for real robots due to its reliance on extensive trial and error. 
                 To overcome these challenges, I focus on developing scalable robot learning methods that combine imitation learning and reinforcement learning.
                Representative papers are <span class="highlight">highlighted</span>.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tbody>
            <!-- <tr style="background-color: #ffffd0;"> -->
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <div style="display: flex;">
                  <div style="flex: 0 0 25%; max-width: 25%;">
                    <img src='images/toperl_critic.png' style="width: 100%; max-width: 100%;">
                  </div>
                  <div style="flex: 0 0 75%; max-width: 75%; padding-left: 20px;">
                    <a href="https://arxiv.org/abs/2410.09536">
                      <papertitle>TOP-ERL: Transformer-based Off-Policy Episodic Reinforcement Learning</papertitle>
                    </a>
                    <br>
                    Ge Li, Dong Tian, <strong>Hongyi Zhou</strong>, Xinkai Jiang,
                    Rudolf Lioutikov, Gerhard Neumann<br>
                    <br>
                    Preprint, Under Review
                    <br>
                    <a href="https://arxiv.org/abs/2410.09536">arXiv</a>
                    <p></p>
                    <p>
                    This work introduces Transformer-based Off-Policy Episodic Reinforcement Learning (TOP-ERL), 
                    a novel algorithm that enables off-policy updates in the ERL framework. ERL methods are often constrained to on-policy frameworks due to the difficulty of evaluating state-action values for entire action sequences, limiting their sample efficiency and preventing the use of more efficient off-policy architectures. 
                    TOP-ERL addresses this shortcoming by segmenting long action sequences and estimating the state-action values for each segment using a transformer-based critic architecture alongside an n-step return estimation.
                    </p>
                  </div>
                </div>
              </td>
            </tr>
            <tr style="background-color: #ffffd0;">
            <!-- <tr> -->
              <td style="padding:20px;width:100%;vertical-align:middle">
                <div style="display: flex;">
                  <div style="flex: 0 0 25%; max-width: 25%;">
                    <img src='images/vdd_toytask.gif' style="width: 100%; max-width: 100%;">
                  </div>
                  <div style="flex: 0 0 75%; max-width: 75%; padding-left: 20px;">
                    <a href="https://arxiv.org/abs/2406.12538">
                      <papertitle>Variational Distillation of Diffusion Policies into Mixture of Experts</papertitle>
                    </a>
                    <br>
                    <strong>Hongyi Zhou</strong>,
                    Denis Blessing,
                    Ge Li,
                    Onur Celik,
                    Xiaogang Jia,
                    <a href="http://rudolf.intuitive-robots.net/">Gerhard Neumann</a> <br>
                    <a href="https://alr.anthropomatik.kit.edu/21_65.php">Rudolf Lioutikov</a> <br>
                    <br>
                    NeurIPS 2024, Poster
                    <br>
                    <a href="https://intuitive-robots.github.io/vdd-website/">Project Page</a>
                    /
                    <a href="https://github.com/intuitive-robots/vdd">Code</a>
                    /
                    <a href="https://arxiv.org/abs/2406.12538">arXiv</a>
                    <p></p>
                    <p>
                   We introduce Variational Diffusion Distillation (VDD), a novel method that distills denoising diffusion policies into Mixtures of Experts (MoE) through variational inference. 
                   VDD is the first method that distills pre-trained diffusion models into MoE models, and hence, combines the expressiveness of Diffusion Models with the benefits of Mixture Models. 
                   Specifically, VDD leverages a decompositional upper bound of the variational objective that allows the training of each expert separately, resulting in a robust optimization scheme for MoEs. 
                    </p>
                  </div>
                </div>
              </td>
            </tr>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <div style="display: flex;">
                  <div style="flex: 0 0 25%; max-width: 25%;">
                    <img src='images/dynamic_hit.gif' style="width: 100%; max-width: 100%;">
                  </div>
                  <div style="flex: 0 0 75%; max-width: 75%; padding-left: 20px;">
                    <a href="https://arxiv.org/pdf/2411.05718">
                      <papertitle>A Retrospective on the Robot Air Hockey Challenge: Benchmarking Robust, Reliable, and Safe Learning Techniques for Real-world Robotics</papertitle>
                    </a>
                    <br>
                    Puze Liu, Jonas G√ºnster, Niklas Funk, Simon Gr√∂ger, Dong Chen, Haitham Bou-Ammar, Julius Jankowski, Ante Mariƒá, Sylvain Calinon, Andrej Orsula, Miguel Olivares-Mendez, 
                    <strong>Hongyi Zhou</strong>, 
                    Rudolf Lioutikov, Gerhard Neumann, Amarildo Likmeta Amirhossein Zhalehmehrabi, Thomas Bonenfant, Marcello Restelli, Davide Tateo, Ziyuan Liu, Jan Peters
                    <br><br>
                    NeurIPS 2024 Dataset and Benchmarks Track, Poster 
                    <br>
                    <a href="https://arxiv.org/pdf/2411.05718">arXiv</a>
                    <p></p>
                    <p>
                      When deploying learning-based approaches on real robots, extra effort is required to address the challenges posed by various real-world factors. To investigate the key factors influencing real-world deployment and to encourage original solutions from different researchers, we organized the Robot Air Hockey Challenge at the NeurIPS 2023 conference. We selected the air hockey task as a benchmark, encompassing low-level robotics problems and high-level tactics. Different from other machine learning-centric benchmarks, participants need to tackle practical challenges in robotics, such as the sim-to-real gap, low-level control issues, safety problems, real-time requirements, and the limited availability of real-world data.
                    </p>
                  </div>
                </div>
              </td>
            </tr>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <!-- <tr style="background-color: #ffffd0;"> -->
                  <td style="padding:20px;width:100%;vertical-align:middle">
                    <div style="display: flex;">
                      <div style="flex: 0 0 25%; max-width: 25%;">
                        <img src='images/mail_rep.jpg' style="width: 100%; max-width: 100%;">
                      </div>
                      <div style="flex: 0 0 75%; max-width: 75%; padding-left: 20px;">
                        <a href="https://intuitive-robots.github.io/mdt_policy/">
                          <papertitle>MaIL: Improving Imitation Learning with Selective State Space Models</papertitle>
                        </a>
                        <br>
                        Xiaogang Jia, Qian Wang, Atalay Donat, Bowen Xing, Ge Li, <strong>Hongyi Zhou</strong>, Onur Celik, Denis Blessing, Rudolf Lioutikov, Gerhard Neumann
                        <br>
                        <br>
                        <em>Conference of Robot Learning (CoRL) 2024, Poster</em>
                        <br>
                            <a href="https://xiaogangjia.github.io/mail_website/">Project Page</a> 
                            /
                            <a href="https://github.com/ALRhub/MaIL">Code </a>
                            /
                        <a href="https://arxiv.org/abs/2406.08234">arXiv</a>
                        <p></p>
                        <p>
                         We introduce Mamba Imitation Learning (MaIL), 
                         a novel imitation learning (IL) architecture that offers a computationally efficient alternative to state-of-the-art (SoTA) Transformer policies.
                         MaIL leverages Mamba as a backbone and introduces a formalism that allows using Mamba in the encoder-decoder structure. This formalism makes it a versatile architecture that can be used as a standalone policy or as part of a more advanced architecture, such as a diffuser in the diffusion process. Extensive evaluations on the LIBERO IL benchmark and three real robot experiments show that MaIL: i) outperforms Transformers in all LIBERO tasks, ii) achieves good performance even with small datasets, iii) is able to effectively process multi-modal sensory inputs, iv) is more robust to input noise compared to Transformers.
                        </p>
                      </div>                      
                    </div>
                  </td>
                </tr>
                <!-- Second Entry -->
                <tr>
                  <td style="padding:20px;width:100%;vertical-align:middle">
                    <div style="display: flex;">
                      <div style="flex: 0 0 25%; max-width: 25%;">
                        <img src='images/tce_framework.png' style="width: 100%; max-width: 100%;">
                      </div>
                      <div style="flex: 0 0 75%; max-width: 75%; padding-left: 20px;">
                        <a href="https://openreview.net/pdf?id=6pPYRXKPpw">
                          <papertitle>Open the Black Box: Step-based Policy Updates for Temporally-Correlated Episodic Reinforcement Learning
                          </papertitle></a>
                        <br>
                        Ge Li, <strong>Hongyi Zhou</strong>, Dominik Roth, Serge Thilges, Fabian Otto, Rudolf Lioutikov, Gerhard Neumann  
                        <br><br>
                          ICLR 2024, Poster
                          <br>
                          <a href="https://arxiv.org/abs/2401.11437">arXiv</a>
                          <p></p>
                          <p>
                            In this work, we introduce a novel ERL algorithm, Temporally-Correlated Episodic RL (TCE), which effectively utilizes step information in episodic policy updates, opening the 'black box' in existing ERL methods while retaining the smooth and consistent exploration in parameter space. TCE synergistically combines the advantages of step-based and episodic RL, achieving comparable performance to recent ERL methods while maintaining data efficiency akin to state-of-the-art (SoTA) step-based RL. 
                          </p>
                      </div>
                    </div>
                  </td>
                </tr>
              </tbody>
            </table>
            
            <tbody>
              <tr style="background-color: #ffffd0;">
                <td style="padding:20px;width:100%;vertical-align:middle">
                    <div style="display: flex;">
                      <div style="flex: 0 0 25%; max-width: 25%;">
                        <img src='images/mp3_tt.gif' style="width: 100%; max-width: 100%;">
                      </div>
                        <div style="flex: 0 0 75%; max-width: 75%; padding-left: 20px;">
                            <a href="https://intuitive-robots.github.io/mp3_website/">
                                <papertitle>MP3: Movement Primitive-Based (Re-)Planning Policy</papertitle>
                            </a>
                            <br>
                            Fabian Otto*, <strong>Hongyi Zhou*</strong>, Onur Celik, Ge Li, Rudolf Lioutikov, Gerhard Neumann
                            <br> * Equal contribution
                            <br>
                            <br>
                            <em>Under Review, 2023
                            <br>
                            <a href="https://intuitive-robots.github.io/mp3_website/">project page</a> 
                            /
                            <a href="https://arxiv.org/pdf/2306.12729">arXiv</a>
                            <p></p>
                            <p>
                            We introduce a novel deep reinforcement learning (RL) approach called Movement Primitive-based Planning Policy (MP3). By integrating movement primitives (MPs) into the deep RL
                            framework, MP3 enables the generation of smooth trajectories throughout the whole learning process while effectively learning from sparse and non-Markovian rewards. Additionally,
                            MP3 maintains the capability to adapt to changes in the environment during execution.
                            </p>
                        </div>
                    </div>
                </td>
            </tr>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <div style="display: flex;">
                  <div style="flex: 0 0 25%; max-width: 25%;">
                    <img src='images/bbrl_framework.png' style="width: 100%; max-width: 100%;">
                  </div>
                  <div style="flex: 0 0 75%; max-width: 75%; padding-left: 20px;">
                    <a href="https://proceedings.mlr.press/v205/otto23a.html">
                      <papertitle> Deep Black-Box Reinforcement Learning with Movement Primitives
                      </papertitle>
                    </a>
                    <br>
                    Fabian Otto, Onur Celik, <strong>Hongyi Zhou</strong>, Hanna Ziesche, Vien Anh Ngo, Gerhard Neumann
                    <br>
                    <br>
                    <em> Conference of Robot Learning (CoRL) </em> 2022, Poster
                    <br>
                    <a href="https://arxiv.org/pdf/2210.09622">arXiv</a>
                    <p></p>
                    <p>
                    In this paper, we present a new algorithm for deep ERL.
                    It is based on differentiable trust region layers, a successful on-policy deep RL
                    algorithm. These layers allow us to specify trust regions for the policy update that
                    are solved exactly for each state using convex optimization, which enables poli-
                    cies learning with the high precision required for the ERL.
                      </p>
              </td>
            </tr>
            <!-- <tr style="background-color: #ffffd0;"> -->
              <td style="padding:20px;width:100%;vertical-align:middle">
                <div style="display: flex;">
                    <div style="flex: 0 0 25%; max-width: 25%;">
                        <img src='images/hiro.gif' style="width: 100%; max-width: 100%;">
                    </div>
                    <div style="flex: 0 0 75%; max-width: 75%; padding-left: 20px;">
                        <a href="https://ieeexplore.ieee.org/abstract/document/9981740">
                  <papertitle>HIRO: Heuristics Informed Robot Online Path Planning Using Pre-computed Deterministic Roadmaps</papertitle>
                </a>
                <br>
                Xi Huang, Gergely S√≥ti, <strong>Hongyi Zhou</strong>, Christoph Ledermann, Bj√∂rn Hein, Torsten Kr√∂ger
                <br>
                <br>
                <em>2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2022
                <br>
                <a href="https://ieeexplore.ieee.org/abstract/document/9981740">Paper Link</a>
                /
                <a href="https://arxiv.org/pdf/2410.20279">arXiv</a>
                <p></p>
                <p>
                  This paper introduces Heuristics Informed Robot Online Path Planning (HIRO). 
                  Dividing robot environments into static and dynamic elements, 
                  we use the static part for initializing a deterministic roadmap, 
                  which provides a lower bound of the final path cost as informed heuristics for fast path-finding. 
                  These heuristics guide a search tree to explore the roadmap during runtime. 
                  The search tree examines the edges using a fuzzy collision checking concerning the dynamic environment. 
                  Finally, the heuristics tree exploits knowledge fed back from the fuzzy collision checking module and updates the lower bound for the path cost. 
              </p>
              </td>
            </tr>


</tbody>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
      <td style="padding:0px; vertical-align: middle;">
          <br>
          <p style="text-align:right;font-size:small;">
              The website is based on the code from <a href="https://github.com/jonbarron/jonbarron_website">source code</a>!
          </p>
      </td>
  </tr>
</tbody></table>
</body>

</html>
